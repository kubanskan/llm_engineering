{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from openai import OpenAI\n",
    "import pandas as pd \n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "from IPython.display import Markdown, display, update_display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85d6959-2424-4da7-9b37-6786b3a4b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc5984d-7422-45bc-ae31-e988dd58cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6f9722-120e-4713-846b-7874605355fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are a data science expert who can explain concepts and code in an easy way to a layman.\\ \n",
    "Explain the following technical question clearly and in depth, as if teaching a smart beginner. \\ \n",
    "Break down complex concepts, include relevant examples or code if needed, and avoid unnecessary jargon. \\ \n",
    "Keep the answer concise and focused. Avoid overly long explanations.  \\ \n",
    "Please answer using proper Markdown formatting.\\ \n",
    "When providing code examples, always enclose the code in triple backticks with the language specified, like:\n",
    "```python\n",
    "# Your python code here\n",
    "print(\"Hello, world!\")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4d93e6-e8fd-4b2f-9784-9e367dbcd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt(text):\n",
    "    return \"Please explain this:\" + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b43a0d-899a-4041-9cda-ed902b5c0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_me(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = MODEL_GPT,\n",
    "        messages= [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_user_prompt(question)}\n",
    "        ],\n",
    "         stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### What is Accuracy in Data Science?\n",
       "\n",
       "**Accuracy** is a metric that measures the performance of a classification model. It tells us how well the model’s predictions match the actual labels. In simple terms, it helps us understand how many times our model was correct compared to the total number of predictions made.\n",
       "\n",
       "Accuracy is calculated using the following formula:\n",
       "\n",
       "\\[ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}} \\]\n",
       "\n",
       "- **Correct Predictions**: This includes both true positives (the model correctly predicted the positive class) and true negatives (the model correctly predicted the negative class).\n",
       "- **Total Predictions**: This is the sum of true positives, true negatives, false positives (incorrect positive predictions), and false negatives (incorrect negative predictions).\n",
       "\n",
       "### Example\n",
       "\n",
       "Let's say we have a model that classifies whether emails are spam or not. If our model makes 100 predictions and gets 90 of them correct, we can calculate the accuracy.\n",
       "\n",
       "### Python Code to Calculate Accuracy\n",
       "\n",
       "Below is a Python example to calculate accuracy using the test results of a model:\n",
       "\n",
       "python\n",
       "# Sample data for actual and predicted results\n",
       "actual = [1, 0, 1, 1, 0, 1, 0, 1, 0, 0]  # Actual labels (1 = spam, 0 = not spam)\n",
       "predicted = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]  # Model predictions\n",
       "\n",
       "# Function to calculate accuracy\n",
       "def calculate_accuracy(actual, predicted):\n",
       "    correct_predictions = sum(a == p for a, p in zip(actual, predicted))\n",
       "    total_predictions = len(actual)\n",
       "    accuracy = correct_predictions / total_predictions\n",
       "    return accuracy\n",
       "\n",
       "# Calculate accuracy\n",
       "accuracy = calculate_accuracy(actual, predicted)\n",
       "print(f\"Accuracy: {accuracy:.2f}\")  # Print accuracy as a decimal\n",
       "\n",
       "\n",
       "### Explanation of the Code\n",
       "\n",
       "1. **Sample Data**: We define two lists, `actual` and `predicted`, to represent the true labels and the model's predictions.\n",
       "2. **Function**: We create a function `calculate_accuracy` that:\n",
       "   - Counts how many predictions were correct by comparing each element in the `actual` and `predicted` lists.\n",
       "   - Calculates the total number of predictions.\n",
       "   - Uses the formula for accuracy to return the accuracy as a decimal (e.g., 0.90 for 90%).\n",
       "3. **Output**: Finally, we print the accuracy.\n",
       "\n",
       "In this example, the accuracy gives us a clear measure of how well our spam detection model is performing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_me('what is accuracy? show me python code to calculate this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc26205-9487-4555-8db0-c9527f363e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## What is One-Shot Prompting?\n",
       "\n",
       "One-shot prompting is a technique used in natural language processing (NLP), particularly when working with language models like GPT (Generative Pre-trained Transformer). It allows you to give the model a single example to help it understand the task at hand. This can be especially useful when you want to achieve a specific output but don't have the time or resources to provide multiple examples.\n",
       "\n",
       "### How It Works\n",
       "\n",
       "In one-shot prompting, you provide:\n",
       "1. The **task description**: What you want the model to do.\n",
       "2. A **single example**: An input and the expected output.\n",
       "\n",
       "The model then uses this single example to generate the desired responses for new inputs that follow the same pattern.\n",
       "\n",
       "### Example\n",
       "\n",
       "Let's say you want the model to translate English sentences into French. Instead of giving the model many examples of translations, you can just provide one:\n",
       "\n",
       "**Prompt:**\n",
       "\n",
       "Translate the following English sentence to French:\n",
       "\"Hello, how are you?\"\n",
       "French: \"Bonjour, comment ça va?\"\n",
       "\n",
       "\n",
       "#### Task\n",
       "\n",
       "Now, you can give another sentence, and the model will understand what you want it to do:\n",
       "\n",
       "**New Input:**\n",
       "\n",
       "\"What's your name?\"\n",
       "\n",
       "\n",
       "#### Expected Response\n",
       "\n",
       "If the model is working correctly with one-shot prompting, you would expect it to respond with:\n",
       "\n",
       "\"Comment vous appelez-vous?\"\n",
       "\n",
       "\n",
       "### Why Use One-Shot Prompting?\n",
       "\n",
       "- **Efficiency**: It saves time since you only need to provide one example.\n",
       "- **Flexibility**: It allows the model to perform a variety of tasks without extensive retraining.\n",
       "- **Simplicity**: It’s easier for users who may not have expertise in working with language models.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "One-shot prompting is a powerful way to guide a language model using just one example. This technique helps to quickly establish the context and expected format for the model’s responses, making it a handy tool for many applications in natural language processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_me('what is one shot prompting?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93f97383-099c-4039-b1ce-0ab1d8393fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_me(question):\n",
    "    response = ollama.chat(model=MODEL_LLAMA, messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_user_prompt(question)}\n",
    "        ] )\n",
    "    return Markdown(response.message.content)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f15fa23-d4da-4f6d-b058-c24ec59a5322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**One-Shot Prompting**\n",
       "=======================\n",
       "\n",
       "One-shot prompting is a technique used in natural language processing (NLP) and artificial intelligence (AI) to improve the performance of language models. In traditional NLP, models are trained on large datasets and fine-tuned for specific tasks, such as question-answering or text generation.\n",
       "\n",
       "**The Problem**\n",
       "---------------\n",
       "\n",
       "However, when faced with a new, unseen prompt, these models often struggle to generate relevant responses. This is because they have not seen the exact input before and lack context.\n",
       "\n",
       "**One-Shot Prompting Solution**\n",
       "-----------------------------\n",
       "\n",
       "One-shot prompting addresses this issue by providing a model with only one example of the desired output for a given input. The idea is that the model can learn to generalize from this single example, allowing it to generate relevant responses for similar inputs.\n",
       "\n",
       "**How it Works**\n",
       "----------------\n",
       "\n",
       "Here's an overview of the process:\n",
       "\n",
       "1. **Training**: The language model is trained on a large dataset using traditional NLP methods.\n",
       "2. **Fine-tuning**: The model is fine-tuned for a specific task, such as question-answering or text generation.\n",
       "3. **One-shot prompting**: A single example of the desired output is provided for a given input. This example is used to guide the model's learning process.\n",
       "\n",
       "**Example Code**\n",
       "```python\n",
       "import torch\n",
       "\n",
       "# Define a simple language model\n",
       "class LanguageModel(torch.nn.Module):\n",
       "    def __init__(self):\n",
       "        super(LanguageModel, self).__init__()\n",
       "        self.fc1 = torch.nn.Linear(10, 128)\n",
       "        self.fc2 = torch.nn.Linear(128, 10)\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = torch.relu(self.fc1(x))\n",
       "        x = self.fc2(x)\n",
       "        return x\n",
       "\n",
       "# Initialize the language model\n",
       "model = LanguageModel()\n",
       "\n",
       "# Provide a single example of the desired output for a given input\n",
       "example_input = \"What is the capital of France?\"\n",
       "example_output = \"Paris\"\n",
       "\n",
       "# Fine-tune the model on this single example\n",
       "criterion = torch.nn.CrossEntropyLoss()\n",
       "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
       "for epoch in range(10):\n",
       "    optimizer.zero_grad()\n",
       "    output = model(example_input)\n",
       "    loss = criterion(output, example_output)\n",
       "    loss.backward()\n",
       "    optimizer.step()\n",
       "\n",
       "# Now the model is fine-tuned for one-shot prompting\n",
       "```\n",
       "In this example, we define a simple language model and provide it with a single example of the desired output for a given input. The model is then fine-tuned on this single example using a cross-entropy loss function and stochastic gradient descent optimizer.\n",
       "\n",
       "**Advantages**\n",
       "--------------\n",
       "\n",
       "One-shot prompting offers several advantages over traditional NLP methods:\n",
       "\n",
       "* **Improved performance**: Models can generate relevant responses for similar inputs.\n",
       "* **Reduced training data requirements**: Only one example is required to fine-tune the model.\n",
       "* **Increased efficiency**: Fine-tuning is faster and more efficient than traditional training methods.\n",
       "\n",
       "However, one-shot prompting also has some limitations and challenges. For example:\n",
       "\n",
       "* **Lack of generalizability**: Models may not generalize well to unseen inputs or tasks.\n",
       "* **Dependence on high-quality examples**: The quality of the single example provided can significantly impact model performance.\n",
       "\n",
       "**Conclusion**\n",
       "----------\n",
       "\n",
       "One-shot prompting is a powerful technique for improving language model performance, especially when combined with fine-tuning and other NLP methods. By providing models with only one example of the desired output, we can enable them to generalize to similar inputs and improve their overall performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_me('what is one shot prompting?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
